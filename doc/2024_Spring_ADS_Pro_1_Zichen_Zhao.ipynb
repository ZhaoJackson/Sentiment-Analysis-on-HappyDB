{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6376e6-55f3-49d7-8227-07832aeae69f",
   "metadata": {},
   "source": [
    "# Proj1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acff64a-bc3e-4b3f-9667-2317052b5d3f",
   "metadata": {},
   "source": [
    "## Load all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23aa5fb-4411-4746-9ca1-a3b3aedb71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de11ef5b-f7d0-41f6-bc5b-3a46e2c2d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jacksonzhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jacksonzhao/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text Simple Preprocessing\n",
    "def simple_preprocess(text):\n",
    "    \"\"\"\n",
    "    Perform simple preprocessing on the given text.\n",
    "    - Convert text to lowercase.\n",
    "    - Remove non-alphabetic characters, keeping only letters and spaces.\n",
    "    - Split text into individual words.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The text to be preprocessed.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of preprocessed words from the text.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert text into lowercase\n",
    "    text = ''.join(char for char in text if char.isalpha() or char.isspace())  # Remove non-alphabetic characters\n",
    "    words = text.split()  # Split text into words\n",
    "    return words\n",
    "\n",
    "def process_data(data_path, age_column, text_column, gender_prefix): # proces gender-specific data for analysis\n",
    "    \"\"\"\n",
    "     Process gender-specific data including text preprocessing, sentiment analysis, and age normalization.\n",
    "     Parameters:\n",
    "     - data_path (str): Path to the CSV file containing the data.\n",
    "     - age_column (str): Name of the column containing age information.\n",
    "     - text_column (str): Name of the column containing text data to be processed.\n",
    "     - gender_prefix (str): Prefix to distinguish between male and female data.\n",
    "     Returns:\n",
    "     - Tuple of processed results: most_common_filtered_words, sentiment_word_freq_by_valid_age\n",
    "    \"\"\"\n",
    "\n",
    "    # data loading\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    data_df[f'{gender_prefix}_processed_text'] = data_df[text_column].apply(simple_preprocess) # Text Preprocessing by converting text to lowercase, removing non-alphabetic characters, and tokenizing\n",
    "\n",
    "    # Calculate word frequency and identify most common words\n",
    "    all_words = [word for text in data_df[f'{gender_prefix}_processed_text'] for word in text]\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in all_words if word not in english_stopwords]\n",
    "    filtered_word_freq = Counter(filtered_words)\n",
    "    most_common_filtered_words = filtered_word_freq.most_common(20)\n",
    "\n",
    "    # Instantiate VADER SentimentIntensityAnalyzer: focus on sentiment words to understand emotional tone of text.\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    vader_lexicon = sia.lexicon\n",
    "    sentiment_words_from_filtered = [word for word in filtered_words if word in vader_lexicon]\n",
    "    unique_sentiment_words = list(set(sentiment_words_from_filtered))\n",
    "\n",
    "    # Applying age normalization\n",
    "    data_df[f'normalized_age_{gender_prefix}'] = data_df[age_column].apply(normalize_age)\n",
    "    valid_age_data_df = data_df.dropna(subset=[f'normalized_age_{gender_prefix}'])\n",
    "    grouped_texts = valid_age_data_df.groupby(f'normalized_age_{gender_prefix}')[f'{gender_prefix}_processed_text']\n",
    "    age_grouped_text_valid = grouped_texts.apply(lambda texts: ' '.join(' '.join(text) for text in texts))\n",
    "\n",
    "    # Word frequency analysis with valid age groups\n",
    "    word_freq_by_valid_age = {}\n",
    "\n",
    "    for age, text in age_grouped_text_valid.items():\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in english_stopwords]\n",
    "        word_freq = Counter(filtered_words)\n",
    "        word_freq_by_valid_age[age] = word_freq\n",
    "\n",
    "    # Analyzing sentiment word frequency by age\n",
    "    sentiment_word_freq_by_valid_age = pd.DataFrame({\n",
    "        word: [word_freq_by_valid_age[age][word] for age in word_freq_by_valid_age] for word in unique_sentiment_words\n",
    "    }, index=word_freq_by_valid_age.keys())\n",
    "\n",
    "    return data_df, most_common_filtered_words, sentiment_word_freq_by_valid_age\n",
    "\n",
    "# Ensure stopwords and VADER's lexicon are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Age Normalization\n",
    "def normalize_age(age):\n",
    "    \"\"\"\n",
    "    Function to normalize age values.\n",
    "    Returns None for non-numeric or invalid age values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(float(age))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# Combine age normalization and basic stopwords\n",
    "def process_data_for_age_with_basic_stopwords_corrected(df, text_column, age_column):\n",
    "    \"\"\"\n",
    "    Process data to calculate word frequencies by age, excluding basic stopwords, and identify top 10 words by percentage.\n",
    "    Correctly handles non-numeric age values by excluding them from the analysis.\n",
    "    \"\"\"\n",
    "    # Apply simple preprocess and normalize age with exclusion of non-numeric values\n",
    "    df['processed_text'] = df[text_column].apply(simple_preprocess)\n",
    "    df['normalized_age'] = df[age_column].apply(lambda x: np.nan if not str(x).isdigit() else int(float(x)))\n",
    "\n",
    "    # Drop rows with NaN ages\n",
    "    valid_data_df = df.dropna(subset=['normalized_age'])\n",
    "\n",
    "    # Aggregate texts by age\n",
    "    aggregated_texts_by_age = valid_data_df.groupby('normalized_age')['processed_text'].agg(sum)\n",
    "\n",
    "    # Calculate word frequencies and percentages\n",
    "    word_freq_percentage_by_age = {}\n",
    "    for age, texts in aggregated_texts_by_age.items():\n",
    "        english_stopwords = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in texts if word not in english_stopwords]\n",
    "        word_freq = Counter(filtered_words)\n",
    "        total_words = sum(word_freq.values())\n",
    "        word_freq_percentage = {word: (count / total_words) * 100 for word, count in word_freq.items()}\n",
    "        # Sort words by frequency percentage and get top 10\n",
    "        top_10_words = sorted(word_freq_percentage.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        word_freq_percentage_by_age[age] = top_10_words\n",
    "\n",
    "    return word_freq_percentage_by_age\n",
    "\n",
    "# Extract top 10 words for each age\n",
    "def extract_word_values(word_freq_data, target_word):\n",
    "    \"\"\"\n",
    "    Extracts and returns the values associated with the target word for each age group\n",
    "    in the provided data structure.\n",
    "\n",
    "    Parameters:\n",
    "    - word_freq_data: A dictionary with age groups as keys and lists of (word, value) tuples as values.\n",
    "    - target_word: The word for which values are to be extracted across all age groups.\n",
    "\n",
    "    Returns:\n",
    "    - results_df: A pandas DataFrame with two columns: 'Age' and '{target_word} Value', where each row corresponds\n",
    "      to an age group and its value for the target word. If the target word is not present, the value will be None.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the results\n",
    "    results = {}\n",
    "    \n",
    "    # Iterate over each age group in the data\n",
    "    for age, word_values in word_freq_data.items():\n",
    "        # Initialize the value for the target word as None for each age group\n",
    "        value_for_target_word = None\n",
    "        \n",
    "        # Search for the target word entry\n",
    "        for word, value in word_values:\n",
    "            if word == target_word:\n",
    "                value_for_target_word = value\n",
    "                break  # Stop searching once the target word is found\n",
    "        \n",
    "        # Assign the found value or None to the results dictionary\n",
    "        results[age] = value_for_target_word\n",
    "\n",
    "    # Convert the dictionary to a DataFrame and dynamically name the 'Value' column based on the target word\n",
    "    results_df = pd.DataFrame(list(results.items()), columns=['Age', f'{target_word} Value'])\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return results_df\n",
    "\n",
    "def merge_word_values_by_age(word_freq_data, words):\n",
    "    \"\"\"\n",
    "    Merges the values for a list of words across age groups into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - word_freq_data: A dictionary with age groups as keys and lists of (word, value) tuples as values.\n",
    "    - words: A list of words to extract and merge values for.\n",
    "\n",
    "    Returns:\n",
    "    - merged_results_df: A pandas DataFrame with age groups as rows and each word's values as columns.\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to hold the merged results\n",
    "    merged_results_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each word to extract its values and merge the results\n",
    "    for word in words:\n",
    "        # Extract values for the current word\n",
    "        results_df = extract_word_values(word_freq_data, word)\n",
    "        \n",
    "        # Rename the column to reflect the current word's values\n",
    "        results_df.rename(columns={f'{word} Value': f'{word}_value'}, inplace=True)\n",
    "        \n",
    "        # If it's the first word, initialize the merged DataFrame with the age and word's value\n",
    "        if merged_results_df.empty:\n",
    "            merged_results_df = results_df\n",
    "        else:\n",
    "            # For subsequent words, merge on 'Age' to ensure alignment across age groups\n",
    "            merged_results_df = pd.merge(merged_results_df, results_df, on='Age', how='outer')\n",
    "\n",
    "    # Return the final merged DataFrame\n",
    "    return merged_results_df\n",
    "\n",
    "\n",
    "# visualize the frequency of a specific word across different ages for male and female participants\n",
    "def plot_word_frequency(word, merged_df, output_directory = '../figs/word_frequency_compare/'):\n",
    "\n",
    "    # Check if the word columns exist in the DataFrame, and if not, initialize them to 0\n",
    "    if f'{word}_male' not in merged_df.columns:\n",
    "        merged_df[f'{word}_male'] = 0\n",
    "    if f'{word}_female' not in merged_df.columns:\n",
    "        merged_df[f'{word}_female'] = 0\n",
    "        \n",
    "    # Filling NaN values with 0 for plotting purposes\n",
    "    plot_data = merged_df[['Age', f'{word}_male', f'{word}_female']].fillna(0)\n",
    "    \n",
    "    # Setting the figure size for better visibility, making it wider\n",
    "    plt.figure(figsize=(14, 6))  # Increased width for a wider chart\n",
    "    \n",
    "    # Creating the bar chart\n",
    "    # Setting the position of bars on the X-axis\n",
    "    bar_width = 0.35\n",
    "    r1 = range(len(plot_data))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "    \n",
    "    # Making the plot\n",
    "    plt.bar(r1, plot_data[f'{word}_male'], color='b', width=bar_width, edgecolor='grey', label='Male')\n",
    "    plt.bar(r2, plot_data[f'{word}_female'], color='r', width=bar_width, edgecolor='grey', label='Female')\n",
    "    \n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Age', fontweight='bold')\n",
    "    plt.xticks([r + bar_width/2 for r in range(len(plot_data))], plot_data['Age'], rotation='vertical')  # Rotate x-axis labels vertically\n",
    "    plt.ylabel(f'{word.capitalize()} Frequency', fontweight='bold')\n",
    "    plt.title(f'Comparison of \"{word.capitalize()}\" Frequency by Age and Gender', fontweight='bold')  # Updated title\n",
    "    \n",
    "    # Creating legend & showing the plot\n",
    "    plt.legend()\n",
    "    plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "            \n",
    "    # Define the output path for saving the graph\n",
    "    output_path = os.path.join(output_directory, f'{word}.jpg')\n",
    "    \n",
    "    # Save the plot to the specified output path\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a128e2a2-3662-4416-aac9-813286e0c475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>killers_male</th>\n",
       "      <th>resigns_male</th>\n",
       "      <th>tranquil_male</th>\n",
       "      <th>exhilarating_male</th>\n",
       "      <th>granted_male</th>\n",
       "      <th>lamented_male</th>\n",
       "      <th>frustrations_male</th>\n",
       "      <th>expand_male</th>\n",
       "      <th>fond_male</th>\n",
       "      <th>...</th>\n",
       "      <th>wins_female</th>\n",
       "      <th>stall_female</th>\n",
       "      <th>dwell_female</th>\n",
       "      <th>stops_female</th>\n",
       "      <th>friends_female</th>\n",
       "      <th>disappointment_female</th>\n",
       "      <th>heroin_female</th>\n",
       "      <th>suffered_female</th>\n",
       "      <th>unemployment_female</th>\n",
       "      <th>worthwhile_female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.379562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 3555 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  killers_male  resigns_male  tranquil_male  exhilarating_male  \\\n",
       "0     2           0.0           0.0            0.0                0.0   \n",
       "1     3           NaN           NaN            NaN                NaN   \n",
       "2     4           NaN           NaN            NaN                NaN   \n",
       "3    17           NaN           NaN            NaN                NaN   \n",
       "4    18           0.0           0.0            0.0                0.0   \n",
       "..  ...           ...           ...            ...                ...   \n",
       "70   88           NaN           NaN            NaN                NaN   \n",
       "71   95           NaN           NaN            NaN                NaN   \n",
       "72   98           NaN           NaN            NaN                NaN   \n",
       "73  227           NaN           NaN            NaN                NaN   \n",
       "74  233           NaN           NaN            NaN                NaN   \n",
       "\n",
       "    granted_male  lamented_male  frustrations_male  expand_male  fond_male  \\\n",
       "0            0.0            0.0                0.0          0.0        0.0   \n",
       "1            NaN            NaN                NaN          NaN        NaN   \n",
       "2            NaN            NaN                NaN          NaN        NaN   \n",
       "3            NaN            NaN                NaN          NaN        NaN   \n",
       "4            0.0            0.0                0.0          0.0        0.0   \n",
       "..           ...            ...                ...          ...        ...   \n",
       "70           NaN            NaN                NaN          NaN        NaN   \n",
       "71           NaN            NaN                NaN          NaN        NaN   \n",
       "72           NaN            NaN                NaN          NaN        NaN   \n",
       "73           NaN            NaN                NaN          NaN        NaN   \n",
       "74           NaN            NaN                NaN          NaN        NaN   \n",
       "\n",
       "    ...  wins_female  stall_female  dwell_female  stops_female  \\\n",
       "0   ...          0.0           0.0           0.0           0.0   \n",
       "1   ...          0.0           0.0           0.0           0.0   \n",
       "2   ...          0.0           0.0           0.0           0.0   \n",
       "3   ...          0.0           0.0           0.0           0.0   \n",
       "4   ...          0.0           0.0           0.0           0.0   \n",
       "..  ...          ...           ...           ...           ...   \n",
       "70  ...          0.0           0.0           0.0           0.0   \n",
       "71  ...          0.0           0.0           0.0           0.0   \n",
       "72  ...          0.0           0.0           0.0           0.0   \n",
       "73  ...          0.0           0.0           0.0           0.0   \n",
       "74  ...          0.0           0.0           0.0           0.0   \n",
       "\n",
       "    friends_female  disappointment_female  heroin_female  suffered_female  \\\n",
       "0         0.000000                    0.0            0.0              0.0   \n",
       "1         0.000000                    0.0            0.0              0.0   \n",
       "2         0.000000                    0.0            0.0              0.0   \n",
       "3         0.000000                    0.0            0.0              0.0   \n",
       "4         4.379562                    0.0            0.0              0.0   \n",
       "..             ...                    ...            ...              ...   \n",
       "70        0.000000                    0.0            0.0              0.0   \n",
       "71        0.000000                    0.0            0.0              0.0   \n",
       "72        0.000000                    0.0            0.0              0.0   \n",
       "73        0.000000                    0.0            0.0              0.0   \n",
       "74        0.000000                    0.0            0.0              0.0   \n",
       "\n",
       "    unemployment_female  worthwhile_female  \n",
       "0                   0.0                0.0  \n",
       "1                   0.0                0.0  \n",
       "2                   0.0                0.0  \n",
       "3                   0.0                0.0  \n",
       "4                   0.0                0.0  \n",
       "..                  ...                ...  \n",
       "70                  0.0                0.0  \n",
       "71                  0.0                0.0  \n",
       "72                  0.0                0.0  \n",
       "73                  0.0                0.0  \n",
       "74                  0.0                0.0  \n",
       "\n",
       "[75 rows x 3555 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manipulate data and separated by gender\n",
    "# Define the path for input and output data\n",
    "demographic_df = pd.read_csv('../data/demographic.csv')\n",
    "cleaned_hm_df = pd.read_csv('../data/cleaned_hm.csv')\n",
    "\n",
    "# Merging the datasets on 'wid' (writer ID)\n",
    "merged_data = pd.merge(cleaned_hm_df, demographic_df, on='wid', how='inner')\n",
    "\n",
    "# Filter for participants from the US\n",
    "us_data = merged_data[merged_data['country'] == 'USA']\n",
    "\n",
    "# Create datasets based on gender\n",
    "male_data = us_data[us_data['gender'] == 'm']\n",
    "female_data = us_data[us_data['gender'] == 'f']\n",
    "\n",
    "# Save to csv\n",
    "male_data.to_csv('../output/male_data.csv', index=False)\n",
    "female_data.to_csv('../output/female_data.csv', index=False)\n",
    "\n",
    "# Load male and female data separately\n",
    "male_data_df, male_most_common, sentiment_word_freq_by_valid_age_male = process_data('../output/male_data.csv', 'age', 'cleaned_hm', 'male')\n",
    "female_data_df, female_most_common, sentiment_word_freq_by_valid_age_female = process_data('../output/female_data.csv', 'age', 'cleaned_hm', 'female')\n",
    "\n",
    "# Change words to percentage\n",
    "total_words_per_age_group_male = sentiment_word_freq_by_valid_age_male.sum(axis=1)\n",
    "sentiment_word_freq_percentages_male = sentiment_word_freq_by_valid_age_male.div(total_words_per_age_group_male, axis=0) * 100\n",
    "\n",
    "total_words_per_age_group_female = sentiment_word_freq_by_valid_age_female.sum(axis=1)\n",
    "sentiment_word_freq_percentages_female = sentiment_word_freq_by_valid_age_female.div(total_words_per_age_group_female, axis=0) * 100\n",
    "\n",
    "# List all words\n",
    "words = list(set(list(sentiment_word_freq_percentages_male.columns.to_list() + sentiment_word_freq_percentages_female.columns.to_list())))\n",
    "\n",
    "# prepare and combine sentiment word frequency data from male and female datasets for analysis and visualization.\n",
    "# Step 1: Rename columns for clarity\n",
    "sentiment_word_freq_percentages_male.columns = [f'{col}_male' if col != 'age' else col for col in sentiment_word_freq_percentages_male.columns]\n",
    "sentiment_word_freq_percentages_female.columns = [f'{col}_female' if col != 'age' else col for col in sentiment_word_freq_percentages_female.columns]\n",
    "\n",
    "# Step 2: Set 'age' as index for both DataFrames\n",
    "sentiment_word_freq_percentages_male.index.name = 'Age'\n",
    "sentiment_word_freq_percentages_female.index.name = 'Age'\n",
    "\n",
    "# Step 3: Perform an outer merge on the index (age)\n",
    "merged_df = pd.merge(sentiment_word_freq_percentages_male, sentiment_word_freq_percentages_female, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Step 4: Reset index to convert 'age' into a column\n",
    "merged_df.reset_index(inplace=True)\n",
    "\n",
    "# Step 5: Show the modified DataFrame\n",
    "merged_df['Age'] = merged_df['Age'].astype(int)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac74c44-a605-46fb-992b-911812e22e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word_frequency_compare for target word value and age between male and female\n",
    "for word in words:\n",
    "    plot_word_frequency(word, merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae89f84b-99c0-4ec9-bf51-7bbff8fb6f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/0rx8hpn96fd0480f9ndhwn_00000gn/T/ipykernel_52787/283258583.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['processed_text'] = df[text_column].apply(simple_preprocess)\n",
      "/var/folders/yd/0rx8hpn96fd0480f9ndhwn_00000gn/T/ipykernel_52787/283258583.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['normalized_age'] = df[age_column].apply(lambda x: np.nan if not str(x).isdigit() else int(float(x)))\n",
      "/var/folders/yd/0rx8hpn96fd0480f9ndhwn_00000gn/T/ipykernel_52787/283258583.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['processed_text'] = df[text_column].apply(simple_preprocess)\n",
      "/var/folders/yd/0rx8hpn96fd0480f9ndhwn_00000gn/T/ipykernel_52787/283258583.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['normalized_age'] = df[age_column].apply(lambda x: np.nan if not str(x).isdigit() else int(float(x)))\n"
     ]
    }
   ],
   "source": [
    "# process the merged data with corrected approach for basic stopwords\n",
    "word_freq_percentage_by_age_corrected_total = process_data_for_age_with_basic_stopwords_corrected(merged_data, 'cleaned_hm', 'age')\n",
    "word_freq_percentage_by_age_corrected_male = process_data_for_age_with_basic_stopwords_corrected(male_data, 'cleaned_hm', 'age')\n",
    "word_freq_percentage_by_age_corrected_female = process_data_for_age_with_basic_stopwords_corrected(female_data, 'cleaned_hm', 'age')\n",
    "\n",
    "# Extract word frequency table for total, male, and female\n",
    "word_frequency_total = merge_word_values_by_age(word_freq_percentage_by_age_corrected_total, words)\n",
    "word_frequency_male = merge_word_values_by_age(word_freq_percentage_by_age_corrected_male, words)\n",
    "word_frequency_female = merge_word_values_by_age(word_freq_percentage_by_age_corrected_female, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (text_processing)",
   "language": "python",
   "name": "text_processing_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
